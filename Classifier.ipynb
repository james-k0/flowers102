{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142eeeb79eb83a8a",
   "metadata": {},
   "source": [
    "# Lets import some things"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ad965f1f42def01",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import time\n",
    "from torchsummary import summary\n",
    "import scipy.io\n",
    "import torchviz"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# I found a text file with the classnames",
   "id": "9cea6976381f8884"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "names = [ 'pink primrose','hard-leaved pocket orchid','canterbury bells','sweet pea','english marigold','tiger lily','moon orchid','bird of paradise','monkshood','globe thistle','snapdragon',\"colt's foot\",'king protea','spear thistle','yellow iris','globe-flower','purple coneflower','peruvian lily','balloon flower','giant white arum lily','fire lily','pincushion flower','fritillary','red ginger','grape hyacinth','corn poppy','prince of wales feathers','stemless gentian','artichoke','sweet william','carnation','garden phlox','love in the mist','mexican aster','alpine sea holly','ruby-lipped cattleya','cape flower','great masterwort','siam tulip','lenten rose','barbeton daisy','daffodil','sword lily','poinsettia','bolero deep blue','wallflower','marigold','buttercup','oxeye daisy','common dandelion','petunia','wild pansy','primula','sunflower','pelargonium','bishop of llandaff','gaura','geranium','orange dahlia','pink-yellow dahlia?','cautleya spicata','japanese anemone','black-eyed susan','silverbush','californian poppy','osteospermum','spring crocus','bearded iris','windflower','tree poppy','gazania','azalea','water lily','rose','thorn apple','morning glory','passion flower','lotus','toad lily','anthurium','frangipani','clematis','hibiscus','columbine','desert-rose','tree mallow','magnolia','cyclamen ','watercress','canna lily','hippeastrum ','bee balm','ball moss','foxglove','bougainvillea','camellia','mallow','mexican petunia','bromelia','blanket flower','trumpet creeper','blackberry lily']",
   "id": "705b2dbe912d5495",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d88e0acccf7cb139",
   "metadata": {},
   "source": [
    "# Decide if cuda"
   ]
  },
  {
   "cell_type": "code",
   "id": "73da4c622feeb2aa",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2ad99af88910169",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "c91f24f7d6957763",
   "metadata": {},
   "source": [
    "trainingData = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split = \"train\",\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop((256, 256),scale=(0.9,1.0),),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20), \n",
    "        transforms.ColorJitter(contrast=0.1,brightness=0.12, saturation=0.1,hue=0.05),#becaise the dataset will evidently be sensitve to colour might want to avoid this one\n",
    "        transforms.RandomAffine(degrees=0,translate=(0.1,0.1),scale=(0.9,1.1),shear=0.1),\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor().to(device),\n",
    "        transforms.Normalize([0.432, 0.381, 0.296], [0.258, 0.209, 0.221])\n",
    "    ])\n",
    ")\n",
    "testData = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split= \"test\",\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor().to(device),\n",
    "        transforms.Normalize([0.432, 0.381, 0.296], [0.258,  0.209, 0.221] )\n",
    "    ])\n",
    ")\n",
    "valData = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split = \"val\",\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor().to(device),\n",
    "        transforms.Normalize([0.432, 0.381, 0.296], [0.258,  0.209, 0.221] )\n",
    "    ])\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'training data has: {len(trainingData)} images')\n",
    "print(f'validation data has: {len(valData)} images')\n",
    "print(f'test data has: {len(testData)} images')"
   ],
   "id": "5eea86165b19f949",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Neural Network class",
   "id": "868426731ab0327f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            #1\n",
    "            nn.Conv2d(3, 64, kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            #2\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #3\n",
    "            nn.Conv2d(128,256,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #4\n",
    "            nn.Conv2d(256,512,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            #5\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #6\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            #7\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512,num_classes), \n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))#6.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.features(x)\n",
    "        x=self.avgpool(x)\n",
    "        x=torch.flatten(x,1)\n",
    "        x= self.classifier(x)\n",
    "        return x"
   ],
   "id": "8d59a57a6b65b022",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training loop",
   "id": "b45d3b77ea993113"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device):\n",
    "    cost = nn.CrossEntropyLoss(label_smoothing=0.1).to(device) #choose the cost function\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001) #choose the optimizer\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=8, factor=0.5, mode='min') #choose the scheduler\n",
    "    \n",
    "    best_val_acc = 0.0 #for if model stops on its not best epoch\n",
    "    best_epoch = 0\n",
    "    \n",
    "    loss_per_epoch = [] #return values\n",
    "    val_epochs = []\n",
    "    tra_epochs = []\n",
    "    \n",
    "    quit_early_counter = 0 #quit early if model not improving or accuracy hits 100% for train set\n",
    "    last_epoch_loss = None\n",
    "    \n",
    "    for epoch in range(num_epochs): #iterate over num_epochs\n",
    "        epoch_start = time.time() #track how long epoch took, was just interested\n",
    "        model.train() #some pytorch magic\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        batches = 0\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_dataloader): #per batch evaluate \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = cost(outputs, labels)\n",
    "            loss.backward() #the bit where it all happens\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batches += 1\n",
    "        \n",
    "        avg_loss = running_loss / batches\n",
    "        loss_per_epoch.append(avg_loss)\n",
    "        \n",
    "        val_acc, val_avg_loss = evaluate(model=model, dataloader=val_dataloader, device=device, cost=cost)\n",
    "        val_epochs.append(val_acc)\n",
    "        tra_acc, train_avg_loss = evaluate(model=model, dataloader=train_dataloader, device=device, cost=cost)\n",
    "        tra_epochs.append(tra_acc)\n",
    "        \n",
    "        scheduler.step(val_avg_loss) #scheduler is there to reduce lr when begins to plateau\n",
    "        \n",
    "        \n",
    "        \n",
    "        if last_epoch_loss is not None and abs(last_epoch_loss - avg_loss) < 0.01:\n",
    "            quit_early_counter += 1\n",
    "        else:\n",
    "            quit_early_counter = 0\n",
    "        last_epoch_loss = avg_loss\n",
    "        \n",
    "        epoch_length = time.time() - epoch_start\n",
    "        \n",
    "        print(f'\\nEpoch: {epoch+1}, Num Batches: {batches}, Avg Loss: {avg_loss:.4f}, Epoch Took: {epoch_length:.1f}s, Validation: {val_acc:.3f}% acc, {val_avg_loss:.3f} loss, Training: {tra_acc:.3f}% acc, {train_avg_loss:.3f} loss') #print out all info pe repoch\n",
    "        \n",
    "        if val_acc > best_val_acc: #save best model\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            save(model, \"best_model_checkpoint\")\n",
    "        \n",
    "        if quit_early_counter >= 10: #quit early if needed\n",
    "            print('Validation accuracy hasnt improved over the last 10 epochs. Stopping training.')\n",
    "            break\n",
    "    \n",
    "    #training is now over\n",
    "    print(f'Best validation accuracy: {best_val_acc:.3f}% at epoch {best_epoch+1}')\n",
    "    print(f'Average loss: {avg_loss:.4f}, Training accuracy: {tra_acc:.3f}%, Validation accuracy: {val_acc:.3f}%')\n",
    "    \n",
    "    load(model, \"best_model_checkpoint\", device)  # Load the best model checkpoint\n",
    "    all_eval(model=model, device=device, cost=nn.CrossEntropyLoss())  # Evaluate based on the best validation accuracy\n",
    "    \n",
    "    return np.array(loss_per_epoch), np.array(val_epochs), np.array(tra_epochs)\n"
   ],
   "id": "72560a0c0069c7aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Function to plot some list",
   "id": "d782dcfdf6d119ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_array(array,name):\n",
    "    plt.plot(array, label=f'{name}')  \n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "6f85777e0d9e04a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate the model for some dataloader against some cost function",
   "id": "b07755bb63750bb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate(model, dataloader, device, cost):\n",
    "    model.eval() # set model ot evalutation mode\n",
    "    correct = 0\n",
    "    total =0\n",
    "    total_loss =0.0\n",
    "    with torch.no_grad(): #so that it doesnt calculate gradients \n",
    "        for images, labels in dataloader: #gather images and labels for a batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = cost(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            total_loss += loss.item() *labels.size(0)\n",
    "        acc= 100 *correct/total\n",
    "        avg_loss = total_loss/total\n",
    "    return acc, avg_loss"
   ],
   "id": "e13855c76f6898e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# evaluate on test, validation and training data",
   "id": "fddbfddd10ce7b2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def all_eval(model, device, cost):\n",
    "    accval, _= evaluate(model=model,dataloader=val_dataloader,device=device,cost=cost)\n",
    "    print(f'val acc: {accval:.3f}%')\n",
    "    acctest, _ = evaluate(dataloader=test_dataloader,model=model,cost=cost,device=device)\n",
    "    print(f'test acc: {acctest:.3f}%')\n",
    "    trainacc, _ = evaluate(dataloader=train_dataloader,model=model,cost=cost,device=device)\n",
    "    print(f'train acc: {trainacc:.3f}%')"
   ],
   "id": "c199a21d4f633053",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save model",
   "id": "64ed56e49ddcbfe0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save(model, pathname):\n",
    "    torch.save(model.state_dict(), f'{pathname}.pth')\n",
    "    print(f'Saved model to {pathname}.pth')"
   ],
   "id": "2c094c4c70e7f5dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load model",
   "id": "645b31994f41178c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load(model, pathname ,device):\n",
    "    model.load_state_dict(torch.load(f'{pathname}.pth'))\n",
    "    model.to(device)\n",
    "    print(f'Loaded model from {pathname}.pth')"
   ],
   "id": "eaac39b9aad117d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualise samples",
   "id": "ad0aef06ac79c7ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_samples(dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sample_index = torch.randint(len(dataset), size=(1,)).item()\n",
    "        image, label = dataset[sample_index]\n",
    "\n",
    "        image = image * torch.tensor([0.258, 0.209, 0.221]).view(3, 1, 1) + torch.tensor([0.432, 0.381, 0.296]).view(3, 1, 1)\n",
    "\n",
    "        axes[i].imshow(image.permute(1, 2, 0))\n",
    "        axes[i].set_title(f'{names[label]}')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "visualize_samples(testData)"
   ],
   "id": "9222f5f9f62b5a6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# main loop",
   "id": "ae906685231968d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_CLASSES = 102\n",
    "    LEARNING_RATE = 0.0015\n",
    "    NUM_EPOCHS = 300\n",
    "    \n",
    "    train_dataloader = DataLoader(trainingData, batch_size=BATCH_SIZE, shuffle=True, num_workers=6, prefetch_factor=4,persistent_workers=True)\n",
    "    test_dataloader = DataLoader(testData, batch_size=BATCH_SIZE, shuffle=False, num_workers=6, prefetch_factor=4,persistent_workers=True)\n",
    "    val_dataloader = DataLoader(valData, batch_size=BATCH_SIZE, shuffle=False, num_workers=6,prefetch_factor=4,persistent_workers=True)\n",
    "    \n",
    "    model = NeuralNet(num_classes=NUM_CLASSES).to(device)\n",
    " \n",
    "    training_epoch_losses, val_acc_per, tra_acc_per = train(model=model,train_dataloader=train_dataloader, val_dataloader=val_dataloader, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, device=device)\n",
    "\n",
    "    plot_array(training_epoch_losses,'training epoch losses')\n",
    "    plot_array(val_acc_per,'validation accuracy per epoch')\n",
    "    plot_array(tra_acc_per,'training accuracy per epoch')"
   ],
   "id": "7f7ef997d3d6ae8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot predicted top k for a random flower",
   "id": "27fe3fa5729dddf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_pred(model, dataloader, names, device):\n",
    "    model.eval() \n",
    "    \n",
    "    img_total = len(dataloader.dataset) #so that it iterates over whole thing not one batch\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    rand_index = torch.randint(0, img_total, (1,)).item()\n",
    "    image,label = dataloader.dataset[rand_index]\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        top5_probs, top5_index = torch.topk(probs, 5) #obtain the top 5 predictions\n",
    "    \n",
    "    top5_probs = top5_probs[0].cpu().numpy()\n",
    "    top5_index = top5_index[0].cpu().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    image_display = image.cpu().squeeze().permute(1, 2, 0) \n",
    "    image_display = image_display * torch.tensor([0.258, 0.209, 0.221]).view(1, 1, 3) + torch.tensor([0.432, 0.381, 0.296]).view(1, 1, 3)\n",
    "    image_display = image_display.numpy()\n",
    "    ax.imshow(image_display)\n",
    "    ax.set_title(f'{names[label]}')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    sns.barplot(x=top5_probs, y=[names[i] for i in top5_index], ax=ax, palette='pastel')\n",
    "    ax.set_title('Top 5 Predictions')\n",
    "    ax.set_xlabel('Probability')\n",
    "    plt.show()\n",
    "\n",
    "plot_pred(model, test_dataloader, names, device)"
   ],
   "id": "90ec07217401cdc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# runs a command that converts this notebook to a py script",
   "id": "3b22420e0604364a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def convert():\n",
    "    !jupyter nbconvert --to script Classifier.ipynb"
   ],
   "id": "312d2e044d24b798",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "convert()",
   "id": "71e4753f39579a00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "caf85920ceec3d3b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
