{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142eeeb79eb83a8a",
   "metadata": {},
   "source": [
    "# Lets import some things"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ad965f1f42def01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:58:06.510305Z",
     "start_time": "2024-05-09T02:58:02.123702Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "d88e0acccf7cb139",
   "metadata": {},
   "source": [
    "# Decide if cuda"
   ]
  },
  {
   "cell_type": "code",
   "id": "73da4c622feeb2aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:58:06.516298Z",
     "start_time": "2024-05-09T02:58:06.511301Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "c2ad99af88910169",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:58:06.523436Z",
     "start_time": "2024-05-09T02:58:06.517325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batchsize = 64\n",
    "num_classes = 102\n",
    "learning_rate = 0.0015\n",
    "num_epochs = 50"
   ],
   "id": "7690d8df799a18b7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c91f24f7d6957763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:58:06.599203Z",
     "start_time": "2024-05-09T02:58:06.525425Z"
    }
   },
   "source": [
    "trainingData = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split = \"train\",\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "testData = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split= \"test\",\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "valData = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split = \"val\",\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "bd64623863f268b9",
   "metadata": {},
   "source": "# Get some dataloaders"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:58:06.604236Z",
     "start_time": "2024-05-09T02:58:06.601202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(trainingData, batch_size=batchsize, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(testData, batch_size=batchsize, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_dataloader = DataLoader(valData, batch_size=batchsize, shuffle=False, num_workers=4, pin_memory=True)"
   ],
   "id": "b9a820ebbeb7301",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Neural Network class",
   "id": "868426731ab0327f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:58:06.610689Z",
     "start_time": "2024-05-09T02:58:06.605391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3,32,kernel_size=5,stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),#half size\n",
    "            nn.Conv2d(32,64,kernel_size=5,stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),#half size\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64*64*64 ,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2), #add a bit of randomness for some fun  + generality\n",
    "            nn.Linear(1024,102),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.features(x)\n",
    "        x= x.view(x.size(0),-1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "# i should stop drinking because where did i imagine this up !?"
   ],
   "id": "8d59a57a6b65b022",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model = something",
   "id": "dc3cece99a0a34e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:58:08.289084Z",
     "start_time": "2024-05-09T02:58:06.611729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NeuralNet().to(device,non_blocking=True)\n",
    "cost = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ],
   "id": "3c674a71d42e7ccf",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Actually do the training, needs to print less often",
   "id": "b45d3b77ea993113"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:05:30.649453Z",
     "start_time": "2024-05-09T02:58:08.290081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train():\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        batches = 0\n",
    "        for i, (images,labels) in enumerate(train_dataloader):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(images)\n",
    "            optimizer.zero_grad()\n",
    "            loss = cost(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batches +=1\n",
    "        print(f'Epoch: {epoch+1}, Avg Loss: {running_loss/batches:4f}, Num Batches: {batches}')\n",
    "train()\n",
    "# next steps: the running loss is a bit jank since batches is always 16 it will print the same batch, should be shuffled though but doesnt look like it ? \n",
    "# maybe im confusing shuffles with individual weights"
   ],
   "id": "72560a0c0069c7aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg Loss: 4.626166, Num Batches: 16\n",
      "Epoch: 2, Avg Loss: 4.624361, Num Batches: 16\n",
      "Epoch: 3, Avg Loss: 4.621361, Num Batches: 16\n",
      "Epoch: 4, Avg Loss: 4.618555, Num Batches: 16\n",
      "Epoch: 5, Avg Loss: 4.616388, Num Batches: 16\n",
      "Epoch: 6, Avg Loss: 4.614104, Num Batches: 16\n",
      "Epoch: 7, Avg Loss: 4.612145, Num Batches: 16\n",
      "Epoch: 8, Avg Loss: 4.608122, Num Batches: 16\n",
      "Epoch: 9, Avg Loss: 4.605346, Num Batches: 16\n",
      "Epoch: 10, Avg Loss: 4.602385, Num Batches: 16\n",
      "Epoch: 11, Avg Loss: 4.599293, Num Batches: 16\n",
      "Epoch: 12, Avg Loss: 4.595567, Num Batches: 16\n",
      "Epoch: 13, Avg Loss: 4.592392, Num Batches: 16\n",
      "Epoch: 14, Avg Loss: 4.585939, Num Batches: 16\n",
      "Epoch: 15, Avg Loss: 4.582482, Num Batches: 16\n",
      "Epoch: 16, Avg Loss: 4.577732, Num Batches: 16\n",
      "Epoch: 17, Avg Loss: 4.573315, Num Batches: 16\n",
      "Epoch: 18, Avg Loss: 4.565603, Num Batches: 16\n",
      "Epoch: 19, Avg Loss: 4.560777, Num Batches: 16\n",
      "Epoch: 20, Avg Loss: 4.552306, Num Batches: 16\n",
      "Epoch: 21, Avg Loss: 4.545282, Num Batches: 16\n",
      "Epoch: 22, Avg Loss: 4.534755, Num Batches: 16\n",
      "Epoch: 23, Avg Loss: 4.525997, Num Batches: 16\n",
      "Epoch: 24, Avg Loss: 4.513415, Num Batches: 16\n",
      "Epoch: 25, Avg Loss: 4.500637, Num Batches: 16\n",
      "Epoch: 26, Avg Loss: 4.484775, Num Batches: 16\n",
      "Epoch: 27, Avg Loss: 4.468848, Num Batches: 16\n",
      "Epoch: 28, Avg Loss: 4.453058, Num Batches: 16\n",
      "Epoch: 29, Avg Loss: 4.435671, Num Batches: 16\n",
      "Epoch: 30, Avg Loss: 4.409790, Num Batches: 16\n",
      "Epoch: 31, Avg Loss: 4.382433, Num Batches: 16\n",
      "Epoch: 32, Avg Loss: 4.366959, Num Batches: 16\n",
      "Epoch: 33, Avg Loss: 4.340346, Num Batches: 16\n",
      "Epoch: 34, Avg Loss: 4.308746, Num Batches: 16\n",
      "Epoch: 35, Avg Loss: 4.263494, Num Batches: 16\n",
      "Epoch: 36, Avg Loss: 4.223477, Num Batches: 16\n",
      "Epoch: 37, Avg Loss: 4.186438, Num Batches: 16\n",
      "Epoch: 38, Avg Loss: 4.146662, Num Batches: 16\n",
      "Epoch: 39, Avg Loss: 4.090048, Num Batches: 16\n",
      "Epoch: 40, Avg Loss: 4.033151, Num Batches: 16\n",
      "Epoch: 41, Avg Loss: 3.991519, Num Batches: 16\n",
      "Epoch: 42, Avg Loss: 3.951311, Num Batches: 16\n",
      "Epoch: 43, Avg Loss: 3.902952, Num Batches: 16\n",
      "Epoch: 44, Avg Loss: 3.853703, Num Batches: 16\n",
      "Epoch: 45, Avg Loss: 3.807970, Num Batches: 16\n",
      "Epoch: 46, Avg Loss: 3.735071, Num Batches: 16\n",
      "Epoch: 47, Avg Loss: 3.714122, Num Batches: 16\n",
      "Epoch: 48, Avg Loss: 3.662263, Num Batches: 16\n",
      "Epoch: 49, Avg Loss: 3.600572, Num Batches: 16\n",
      "Epoch: 50, Avg Loss: 3.592186, Num Batches: 16\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Display the training, testing, validation accuracy",
   "id": "b07755bb63750bb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:06:15.041429Z",
     "start_time": "2024-05-09T03:05:30.650452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total =0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "        acc= float(100*correct/total)\n",
    "    return acc\n",
    "print(f'val acc: {evaluate(val_dataloader):.3f}%')\n",
    "print(f'test acc: {evaluate(test_dataloader):.3f}%')\n",
    "print(f'train acc: {evaluate(train_dataloader):.3f}%')"
   ],
   "id": "e13855c76f6898e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 9.804%\n",
      "test acc: 8.733%\n",
      "train acc: 24.118%\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save model",
   "id": "64ed56e49ddcbfe0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:06:15.046850Z",
     "start_time": "2024-05-09T03:06:15.043427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save(pathname):\n",
    "    torch.save(NeuralNet().state_dict(), f'{pathname}.pth')\n",
    "    print(f'Saved model to {pathname}.pth')"
   ],
   "id": "2c094c4c70e7f5dc",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load model",
   "id": "645b31994f41178c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:06:15.053830Z",
     "start_time": "2024-05-09T03:06:15.047845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load(pathname):\n",
    "    model = NeuralNet().to(device)\n",
    "    model.load_state_dict(torch.load(f'{pathname}.pth'))\n",
    "    print(f'Loaded model from {pathname}.pth')"
   ],
   "id": "eaac39b9aad117d5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Command line to convert this notebook to a python file, the reason is for readability of the code from github lol",
   "id": "3b22420e0604364a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:06:18.342013Z",
     "start_time": "2024-05-09T03:06:15.054827Z"
    }
   },
   "cell_type": "code",
   "source": "!jupyter nbconvert --to script Classifier.ipynb",
   "id": "312d2e044d24b798",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Classifier.ipynb to script\n",
      "[NbConvertApp] Writing 5063 bytes to Classifier.py\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# todo possibly do the image display thing/ https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html / tune hyperparams",
   "id": "a2cad47e7f89011c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
