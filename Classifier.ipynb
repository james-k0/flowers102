{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142eeeb79eb83a8a",
   "metadata": {},
   "source": [
    "# Lets import some things"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ad965f1f42def01",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import time\n",
    "from torchsummary import summary\n",
    "import scipy.io"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mat = scipy.io.loadmat('data/flowers-102/imagelabels.mat')\n",
    "print(len(mat['labels'][0])) #it turns out the image labels are numbers and i cant recognise flowers by eye. nvm i found someone helpful online"
   ],
   "id": "6ea7dcae184a9fc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# I found a text file with the classnames",
   "id": "9cea6976381f8884"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "names = [ 'pink primrose','hard-leaved pocket orchid','canterbury bells','sweet pea','english marigold','tiger lily','moon orchid','bird of paradise','monkshood','globe thistle','snapdragon',\"colt's foot\",'king protea','spear thistle','yellow iris','globe-flower','purple coneflower','peruvian lily','balloon flower','giant white arum lily','fire lily','pincushion flower','fritillary','red ginger','grape hyacinth','corn poppy','prince of wales feathers','stemless gentian','artichoke','sweet william','carnation','garden phlox','love in the mist','mexican aster','alpine sea holly','ruby-lipped cattleya','cape flower','great masterwort','siam tulip','lenten rose','barbeton daisy','daffodil','sword lily','poinsettia','bolero deep blue','wallflower','marigold','buttercup','oxeye daisy','common dandelion','petunia','wild pansy','primula','sunflower','pelargonium','bishop of llandaff','gaura','geranium','orange dahlia','pink-yellow dahlia?','cautleya spicata','japanese anemone','black-eyed susan','silverbush','californian poppy','osteospermum','spring crocus','bearded iris','windflower','tree poppy','gazania','azalea','water lily','rose','thorn apple','morning glory','passion flower','lotus','toad lily','anthurium','frangipani','clematis','hibiscus','columbine','desert-rose','tree mallow','magnolia','cyclamen ','watercress','canna lily','hippeastrum ','bee balm','ball moss','foxglove','bougainvillea','camellia','mallow','mexican petunia','bromelia','blanket flower','trumpet creeper','blackberry lily']",
   "id": "705b2dbe912d5495",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d88e0acccf7cb139",
   "metadata": {},
   "source": [
    "# Decide if cuda"
   ]
  },
  {
   "cell_type": "code",
   "id": "73da4c622feeb2aa",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2ad99af88910169",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "c91f24f7d6957763",
   "metadata": {},
   "source": [
    "trainingData = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split = \"train\",\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomResizedCrop((256, 256),antialias=True),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15), \n",
    "        transforms.ColorJitter(contrast=0.08,brightness=0.08, saturation=0.08),\n",
    "        transforms.RandomAffine(degrees=10,translate=(0.1,0.1),scale=(0.9,1.1),shear=0.15),\n",
    "        transforms.GaussianBlur(kernel_size=(3,3),sigma=(0.1,2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.43554732, 0.37773556, 0.28792068], [0.26202053, 0.20857088, 0.21565172])\n",
    "    ])\n",
    ")\n",
    "testData = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split= \"test\",\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.43554732, 0.37773556, 0.28792068], [0.26202053, 0.20857088, 0.21565172])\n",
    "    ])\n",
    ")\n",
    "valData = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split = \"val\",\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.43554732, 0.37773556, 0.28792068], [0.26202053, 0.20857088, 0.21565172])\n",
    "    ])\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'training data has: {len(trainingData)} images')\n",
    "print(f'validation data has: {len(valData)} images')\n",
    "print(f'test data has: {len(testData)} images')"
   ],
   "id": "5eea86165b19f949",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Neural Network class",
   "id": "868426731ab0327f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            #conv1\n",
    "            nn.Conv2d(3,32,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),#128x128\n",
    "            #conv2\n",
    "            nn.Conv2d(32,64,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),#64x64\n",
    "            #conv3\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),#32x32\n",
    "            #conv4\n",
    "            nn.Conv2d(128,256,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),#16x16\n",
    "            #conv5\n",
    "            nn.Conv2d(256,512,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)#8x8\n",
    "        )\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024,num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.features(x)\n",
    "        x= self.global_avg_pool(x)\n",
    "        x= x.view(x.size(0),-1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "id": "8d59a57a6b65b022",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training loop",
   "id": "b45d3b77ea993113"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device):\n",
    "    cost = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_per_epoch = [] # clear all data incase retraining\n",
    "    val_epochs = []\n",
    "    tra_epochs = []\n",
    "    quit_early_counter = 0\n",
    "    last_epoch_loss = None\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        batches = 0\n",
    "        \n",
    "        for i, (images,labels) in enumerate(train_dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            optimizer.zero_grad()\n",
    "            loss = cost(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() #so this line is a sync point for the cpu and gpu commenting it out greatly reduces the training time (up to 25%  in my testing)\n",
    "            batches +=1\n",
    "        \n",
    "        avg_loss = running_loss / batches\n",
    "        loss_per_epoch.append(avg_loss)\n",
    "        \n",
    "        val_acc,val_avg_loss = evaluate(model=model,dataloader=val_dataloader,device=device,cost=cost)\n",
    "        val_epochs.append(val_acc)\n",
    "        tra_acc, train_avg_loss = evaluate(model=model,dataloader=train_dataloader,device=device,cost=cost)\n",
    "        tra_epochs.append(tra_acc)\n",
    "        # scheduler.step(val_loss)\n",
    "        \n",
    "        if last_epoch_loss is not None and abs(last_epoch_loss - avg_loss)<0.01:\n",
    "            quit_early_counter += 1\n",
    "        else:\n",
    "            quit_early_counter = 0\n",
    "        last_epoch_loss = avg_loss\n",
    "        \n",
    "        epoch_length = time.time() - epoch_start\n",
    "        # print(f'\\n Last lr {float(scheduler.get_last_lr()[0])}, Epoch: {epoch+1}')\n",
    "        print(f'Epoch: {epoch+1}, Avg Loss: {avg_loss:4f}, Num Batches: {batches}, Epoch Time: {epoch_length:.2f}, Validation Acc: {val_acc:.3f}%, Training Acc: {tra_acc:.3f}%')\n",
    "        \n",
    "        if quit_early_counter >= 3:\n",
    "            print('val acc isnt improving over last 5 so stop training')\n",
    "            break\n",
    "            \n",
    "    return np.array(loss_per_epoch), np.array(val_epochs)"
   ],
   "id": "72560a0c0069c7aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot the avg loss against epochs",
   "id": "d782dcfdf6d119ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_array(array,name):\n",
    "    plt.plot(array, label=f'{name}')\n",
    "    # plt.plot(validation_epoch_losses,label='Validation Loss')   \n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "6f85777e0d9e04a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Display the training, testing, validation accuracy",
   "id": "b07755bb63750bb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate(model, dataloader, device, cost):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total =0\n",
    "    total_loss =0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = cost(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            total_loss += loss.item() *labels.size(0)\n",
    "        acc= 100 *correct/total\n",
    "        avg_loss = total_loss/total\n",
    "    return acc, avg_loss"
   ],
   "id": "e13855c76f6898e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# evaluate on test, validation and training data",
   "id": "fddbfddd10ce7b2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def all_eval(model, device, cost):\n",
    "    accval, _= evaluate(model=model,device=device,cost=cost,dataloader=test_dataloader)\n",
    "    print(f'val acc: {accval:.3f}%')\n",
    "    acctest, _ = evaluate(dataloader=test_dataloader,model=model,cost=cost,device=device)\n",
    "    print(f'test acc: {acctest:.3f}%')\n",
    "    trainacc, _ = evaluate(dataloader=train_dataloader,model=model,cost=cost,device=device)\n",
    "    print(f'train acc: {trainacc:.3f}%')"
   ],
   "id": "c199a21d4f633053",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save model",
   "id": "64ed56e49ddcbfe0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save(model, pathname):\n",
    "    torch.save(model.state_dict(), f'{pathname}.pth')\n",
    "    print(f'Saved model to {pathname}.pth')"
   ],
   "id": "2c094c4c70e7f5dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load model",
   "id": "645b31994f41178c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load(model, pathname ,device):\n",
    "    model.load_state_dict(torch.load(f'{pathname}.pth'))\n",
    "    model.to(device)\n",
    "    print(f'Loaded model from {pathname}.pth')"
   ],
   "id": "eaac39b9aad117d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualise samples",
   "id": "ad0aef06ac79c7ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_samples(dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Randomly select a sample from the dataset\n",
    "        sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "        image, label = dataset[sample_idx]\n",
    "\n",
    "        # Denormalize the image\n",
    "        image = image * torch.tensor([0.26202053, 0.20857088, 0.21565172]).view(3, 1, 1) + torch.tensor([0.43554732, 0.37773556, 0.28792068]).view(3, 1, 1)\n",
    "        \n",
    "        # Plot the image\n",
    "        axes[i].imshow(image.permute(1, 2, 0))\n",
    "        axes[i].set_title(f'{names[label]}')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset\n",
    "visualize_samples(trainingData)\n",
    "#i did this to sanity check why everything i do achieves a acc of 0.98% aka a perfect guess... and the labels are correct so idk"
   ],
   "id": "9222f5f9f62b5a6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# main loop",
   "id": "ae906685231968d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    BATCH_SIZE = 20\n",
    "    NUM_CLASSES = 102\n",
    "    LEARNING_RATE = 0.0003\n",
    "    NUM_EPOCHS = 100\n",
    "    \n",
    "    train_dataloader = DataLoader(trainingData, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, prefetch_factor=2,persistent_workers=True)\n",
    "    test_dataloader = DataLoader(testData, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    val_dataloader = DataLoader(valData, batch_size=BATCH_SIZE, shuffle=False, num_workers=4,prefetch_factor=2,persistent_workers=True)\n",
    "    # \n",
    "    model = NeuralNet(NUM_CLASSES).to(device)\n",
    "    # summary(model,input_size=(3,224,224))\n",
    "    training_epoch_losses, val_acc_per = train(model=model,train_dataloader=train_dataloader, val_dataloader=val_dataloader, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, device=device)\n",
    "    # \n",
    "    # \n",
    "    plot_array(training_epoch_losses,'training epoch losses')\n",
    "    plot_array(val_acc_per,'validation accuracy per epoch')\n",
    "    all_eval(model=model,device=device,cost=torch.nn.CrossEntropyLoss())"
   ],
   "id": "7f7ef997d3d6ae8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# runs a command that converts this notebook to a py script",
   "id": "3b22420e0604364a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def convert():\n",
    "    !jupyter nbconvert --to script Classifier.ipynb"
   ],
   "id": "312d2e044d24b798",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "convert()",
   "id": "71e4753f39579a00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# todo possibly do the image display thing/ https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html / tune hyperparams",
   "id": "a2cad47e7f89011c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4dc4bba0bbd2482a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
